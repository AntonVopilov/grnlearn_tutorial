{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning of  a simple genetic network in *E. coli*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content here is licensed under a CC 4.0 License. The code in this notebook is released under the MIT license. \n",
    "\n",
    "\n",
    "By Manu Flores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:30.572910Z",
     "start_time": "2019-12-02T22:50:13.720182Z"
    }
   },
   "outputs": [],
   "source": [
    "import grn as g\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import hvplot\n",
    "import hvplot.pandas\n",
    "import holoviews as hv\n",
    "from holoviews import dim, opts\n",
    "import bokeh_catplot\n",
    "import bokeh \n",
    "import bokeh.io\n",
    "from bokeh.io import output_file, save, output_notebook\n",
    "\n",
    "\n",
    "output_notebook()\n",
    "hv.extension('bokeh')\n",
    "np.random.seed(42)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "g.set_plotting_style()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in data RNA-seq dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Story of the data. Citation : y-ome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:31.018852Z",
     "start_time": "2019-12-02T22:50:30.613513Z"
    }
   },
   "outputs": [],
   "source": [
    "# url = 'https://raw.githubusercontent.com/manuflores/grnlearn_tutorial/master/data/palsson_rna_seq.csv'\n",
    "# df = pd.read_csv(url)\n",
    "df = pd.read_csv('../data/palsson_rna_seq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:31.132360Z",
     "start_time": "2019-12-02T22:50:31.048201Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:31.181935Z",
     "start_time": "2019-12-02T22:50:31.148363Z"
    }
   },
   "outputs": [],
   "source": [
    "data_ = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:31.199737Z",
     "start_time": "2019-12-02T22:50:31.191668Z"
    }
   },
   "outputs": [],
   "source": [
    "annot = data_.iloc[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:31.223871Z",
     "start_time": "2019-12-02T22:50:31.207253Z"
    }
   },
   "outputs": [],
   "source": [
    "annot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:31.248002Z",
     "start_time": "2019-12-02T22:50:31.230360Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data_.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start our data analysis pipeline by normalizing and looking for null values ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:31.263106Z",
     "start_time": "2019-12-02T22:50:31.254676Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler as scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:31.348134Z",
     "start_time": "2019-12-02T22:50:31.270170Z"
    }
   },
   "outputs": [],
   "source": [
    "ss = scaler()\n",
    "norm_data = ss.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the data has any null entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:31.931348Z",
     "start_time": "2019-12-02T22:50:31.362361Z"
    }
   },
   "outputs": [],
   "source": [
    "norm_data= pd.DataFrame(norm_data, columns = data.columns)\n",
    "norm_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are none. We can quickly verify this using the `pd.notnull` function from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:31.964936Z",
     "start_time": "2019-12-02T22:50:31.944766Z"
    }
   },
   "outputs": [],
   "source": [
    "np.all(pd.notnull(norm_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, we're good to go ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in PurR regulon datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go ahead and load the PurR regulon datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:31.995861Z",
     "start_time": "2019-12-02T22:50:31.981581Z"
    }
   },
   "outputs": [],
   "source": [
    "purr_regulondb = pd.read_csv('../data/purr_regulon_db.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:32.022882Z",
     "start_time": "2019-12-02T22:50:32.010089Z"
    }
   },
   "outputs": [],
   "source": [
    "purr_hi = pd.read_csv('../data/purr_regulon_hitrn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:32.052654Z",
     "start_time": "2019-12-02T22:50:32.038888Z"
    }
   },
   "outputs": [],
   "source": [
    "print('The RegulonDB has %d nodes and the hiTRN has %d nodes \\\n",
    "for the PurR regulon genetic network respectively.'%(purr_regulondb.shape[0], purr_hi.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the TGs as a `np.array` and get the genes that were discovered by the Palsson Lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:32.070324Z",
     "start_time": "2019-12-02T22:50:32.062760Z"
    }
   },
   "outputs": [],
   "source": [
    "purr_rdb_tgs = np.unique(purr_regulondb.tg.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:32.093311Z",
     "start_time": "2019-12-02T22:50:32.079602Z"
    }
   },
   "outputs": [],
   "source": [
    "len(purr_rdb_tgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:32.112617Z",
     "start_time": "2019-12-02T22:50:32.100497Z"
    }
   },
   "outputs": [],
   "source": [
    "purr_hi_tgs = np.unique(purr_hi.gene.values)\n",
    "\n",
    "purr_hi_tgs = [gene.lower() for gene in purr_hi_tgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:32.136038Z",
     "start_time": "2019-12-02T22:50:32.123506Z"
    }
   },
   "outputs": [],
   "source": [
    "new_purr_tgs = set(purr_hi_tgs) - set(purr_rdb_tgs)\n",
    "\n",
    "new_purr_tgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that indeed the hiTRN has 5 more interactions. Let's see if we can accurately predict this interactions directly from the RNA-seq data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before jumping to applying an ML model to our data, let's proceed to make a simple EDA. As I've said in the presentation the notion that makes this approach biologically plausible is that **genes that are coexpressed are probably corregulated**. A simple proxy for coexpression is correlation across expression conditions. \n",
    "\n",
    "Let's make a couple of plots to see that indeed the test genes that we're looking for are correlated with purr, and if this relationship looks linear. We'll use the Seaborn library in this case because it has a nice feat that allows to embed a statistical function into the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:32.162625Z",
     "start_time": "2019-12-02T22:50:32.144338Z"
    }
   },
   "outputs": [],
   "source": [
    "def corr_plot(data, gene_1, gene_2):\n",
    "    \"\"\"\n",
    "    Scatter plot to devise correlation. \n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    * data(pd.DataFrame): Input dataframe that contains for which to pull out data. \n",
    "    \n",
    "    * gene_x (str): gene_name of the genes to visualize.\n",
    "    \n",
    "    Returns \n",
    "    ---------\n",
    "    * fig (plt.figure) : sns.jointplot hardcoded to be a scatterplot of the genes. \n",
    "    \n",
    "    \"\"\"\n",
    "    gene_1_data  = data[data['gene_name'] == gene_1]\n",
    "    \n",
    "    assert gene_1_data.shape[0] ==1, 'Gene 1 not in dataset'\n",
    "    \n",
    "    gene_1_vals =  gene_1_data.iloc[:, 3:].values.T\n",
    "    \n",
    "    gene_2_data  = data[data['gene_name'] == gene_2]\n",
    "    \n",
    "    assert gene_2_data.shape[0] ==1, 'Gene 2 not in dataset'\n",
    "    \n",
    "    gene_2_vals =  gene_2_data.iloc[:, 3:].values.T\n",
    "    \n",
    "    df_plot = pd.DataFrame({gene_1: gene_1_vals.flatten(),\n",
    "                            gene_2 : gene_2_vals.flatten()})\n",
    "    \n",
    "    plt.figure(figsize = (6, 4))\n",
    "    fig = sns.jointplot(data = df_plot, \n",
    "                  x = gene_1,\n",
    "                  y = gene_2,\n",
    "                  stat_func = pearsonr,\n",
    "                  alpha = 0.5,\n",
    "                  color = 'dodgerblue');\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now iterate over the putative TGs and plot them against PurR. In the following plots, each dot represents the expression level (in [FPKM](https://www.rna-seqblog.com/rpkm-fpkm-and-tpm-clearly-explained/), a proxy for the number of mRNA counts for a given gene) of both genes in a specific expression condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:42.128954Z",
     "start_time": "2019-12-02T22:50:32.170345Z"
    }
   },
   "outputs": [],
   "source": [
    "for new_tg in new_purr_tgs: \n",
    "    \n",
    "    corr_plot(df, 'purr', new_tg);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some, but not all the genes are strongly correlated with PurR. This is normal because the TRN has a lot of feedback so it could be that despite that PurR regulates a given gene, there are potentially other TFs controlling those target genes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter noise using PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis is a widely used technique in unsupervised learning to perform dimensionality reduction. One can also use PCA as a \"noise reduction\" technique because projecting into a (smaller) latent space and reconstructing the dataset from this space with smaller dimensionality forces the algorithm to learn important features of the data. Specifically the latent space (the principal components) will maximize the variance across the dataset. \n",
    "\n",
    "First, let's explore the dimensionality of our RNA-seq dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:42.157605Z",
     "start_time": "2019-12-02T22:50:42.147337Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:42.277872Z",
     "start_time": "2019-12-02T22:50:42.172948Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca = pca.fit(norm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:42.652857Z",
     "start_time": "2019-12-02T22:50:42.284071Z"
    }
   },
   "outputs": [],
   "source": [
    "cum_exp_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# look at it\n",
    "plt.figure(figsize = (6,4))\n",
    "plt.plot(cum_exp_var*100, color = 'dodgerblue') #because LA\n",
    "plt.xlabel('Number of dimensions', fontsize= 16)\n",
    "plt.ylabel('Cumulative variance percentage', fontsize = 16)\n",
    "plt.title('PCA Explained Variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:42.673451Z",
     "start_time": "2019-12-02T22:50:42.662081Z"
    }
   },
   "outputs": [],
   "source": [
    "print('The first five principal components explain %.2f of the variance in the dataset.'%cum_exp_var[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset is of very small dimensionality. We can now project into this subspace that contains 95% of the variance and reconstruct the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:42.750959Z",
     "start_time": "2019-12-02T22:50:42.684187Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(0.95).fit(norm_data)\n",
    "latent = pca.transform(norm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:42.768797Z",
     "start_time": "2019-12-02T22:50:42.757847Z"
    }
   },
   "outputs": [],
   "source": [
    "reconstructed = pca.inverse_transform(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:42.785811Z",
     "start_time": "2019-12-02T22:50:42.779538Z"
    }
   },
   "outputs": [],
   "source": [
    "recon_df= pd.DataFrame(reconstructed, columns = data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:42.840580Z",
     "start_time": "2019-12-02T22:50:42.817811Z"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[:, :2].shape, recon_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:42.878453Z",
     "start_time": "2019-12-02T22:50:42.860008Z"
    }
   },
   "outputs": [],
   "source": [
    "recon_df_ = pd.concat([df.iloc[:, :2], recon_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:42.968465Z",
     "start_time": "2019-12-02T22:50:42.896533Z"
    }
   },
   "outputs": [],
   "source": [
    "recon_df_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize correlation again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the dataset again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:51.657288Z",
     "start_time": "2019-12-02T22:50:42.980231Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for new_tg in new_purr_tgs: \n",
    "    \n",
    "    corr_plot(recon_df_, 'purr', new_tg);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the reconstructed space, we've constrained the data to have a bigger covariance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize in PCA space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we already have the projection of our dataset into a smaller dimension, we can also visualize all of the genes in the first two principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:51.813767Z",
     "start_time": "2019-12-02T22:50:51.672903Z"
    }
   },
   "outputs": [],
   "source": [
    "hv.Points((latent[: , 0], latent[: , 1])).opts(xlabel = 'principal component 1',\n",
    "                                               ylabel = 'principal component 2',\n",
    "                                               color = '#1E90FF', \n",
    "                                               size = 5, \n",
    "                                               alpha = 0.15, \n",
    "                                               padding = 0.1, \n",
    "                                               width = 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot really see a specific structure in the first two components. Maybe a non-linear dimensionality reduction technique such as UMAP could do a better job to get the clusters in higher dimensions. We'll come back to that in the next tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have preprocessed our data we can proceed to annotate it. Specifically we want to label our data for each gene, if its inside the PurR regulon or not. \n",
    "\n",
    "First-off, let's generate our test set. We'll use a helper function that let's us filter from the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:51.837983Z",
     "start_time": "2019-12-02T22:50:51.820220Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_gene_data(data, gene_name_column, test_gene_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extract data from specific genes given a larger dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    \n",
    "    * data (pd.DataFrame): large dataframe from where to filter.\n",
    "    * gene_name_column (str): column to filter from in the dataset.\n",
    "    * test_gene_list (array-like) : a list of genes you want to get. \n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    * gene_profiles (pd.DataFrame) : dataframe with the genes you want\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    gene_profiles = pd.DataFrame()\n",
    "\n",
    "    for gene in data[gene_name_column].values:\n",
    "\n",
    "        if gene in test_gene_list: \n",
    "\n",
    "            df_ = data[(data[gene_name_column] == gene)]\n",
    "\n",
    "            gene_profiles = pd.concat([gene_profiles, df_])\n",
    "    \n",
    "    gene_profiles.drop_duplicates(inplace = True)\n",
    "    \n",
    "    return gene_profiles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a one hot encoded vector that corresponds to being an element of the PurR regulon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:51.860156Z",
     "start_time": "2019-12-02T22:50:51.847021Z"
    }
   },
   "outputs": [],
   "source": [
    "one_hot = [1 if row  in purr_hi_tgs else 0 for  row in  recon_df_['gene_name'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:51.879100Z",
     "start_time": "2019-12-02T22:50:51.869635Z"
    }
   },
   "outputs": [],
   "source": [
    "recon_df_['output'] = one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:51.941486Z",
     "start_time": "2019-12-02T22:50:51.897511Z"
    }
   },
   "outputs": [],
   "source": [
    "recon_df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:51.955679Z",
     "start_time": "2019-12-02T22:50:51.950313Z"
    }
   },
   "outputs": [],
   "source": [
    "test_purr_tgs  = list(new_purr_tgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.042885Z",
     "start_time": "2019-12-02T22:50:51.965812Z"
    }
   },
   "outputs": [],
   "source": [
    "test = get_gene_data(recon_df_, 'gene_name', test_purr_tgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.094433Z",
     "start_time": "2019-12-02T22:50:52.052421Z"
    }
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop these test genes from the reconstructed dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.117618Z",
     "start_time": "2019-12-02T22:50:52.103803Z"
    }
   },
   "outputs": [],
   "source": [
    "recon_df_non_regulon = recon_df_.copy().drop(test.index.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now we can go ahead and add some \"noise\" to our test dataset, in the sense that we need to test if our algorithm can point out negative examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.137090Z",
     "start_time": "2019-12-02T22:50:52.126321Z"
    }
   },
   "outputs": [],
   "source": [
    "noise = recon_df_non_regulon.sample(n = 30, replace = False,\n",
    "                         axis = 0, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge both of this dataframes to get an unbiased test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.154997Z",
     "start_time": "2019-12-02T22:50:52.145843Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_unb = pd.concat([test, noise]) ## unbiased test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.173022Z",
     "start_time": "2019-12-02T22:50:52.164559Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_unb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.189045Z",
     "start_time": "2019-12-02T22:50:52.179282Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_unbiased = df_test_unb.copy().reset_index(drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.235714Z",
     "start_time": "2019-12-02T22:50:52.195644Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_unbiased.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.253383Z",
     "start_time": "2019-12-02T22:50:52.241704Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_unbiased.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.269155Z",
     "start_time": "2019-12-02T22:50:52.260815Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = recon_df_non_regulon.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.351112Z",
     "start_time": "2019-12-02T22:50:52.274802Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.head()\n",
    "df_test_unbiased.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.367213Z",
     "start_time": "2019-12-02T22:50:52.356620Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.shape\n",
    "df_test_unbiased.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.391586Z",
     "start_time": "2019-12-02T22:50:52.373114Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = df_train.iloc[:, 2: -1].values\n",
    "y_train = df_train.iloc[:,  -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.415396Z",
     "start_time": "2019-12-02T22:50:52.399128Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train[:5, :5]\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.434109Z",
     "start_time": "2019-12-02T22:50:52.421904Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = df_test_unbiased.iloc[:, 2:-1].values\n",
    "\n",
    "y_test = df_test_unbiased.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.456506Z",
     "start_time": "2019-12-02T22:50:52.440564Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test[:5, :5]\n",
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance dataset using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.481477Z",
     "start_time": "2019-12-02T22:50:52.468434Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.504878Z",
     "start_time": "2019-12-02T22:50:52.488095Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.665160Z",
     "start_time": "2019-12-02T22:50:52.511220Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#resampling is done on training dataset only\n",
    "X_train_res, y_train_res = SMOTE(random_state = 42).fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.678047Z",
     "start_time": "2019-12-02T22:50:52.671399Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:52.694188Z",
     "start_time": "2019-12-02T22:50:52.686148Z"
    }
   },
   "outputs": [],
   "source": [
    "linear_svm_clf = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:56.633411Z",
     "start_time": "2019-12-02T22:50:52.702199Z"
    }
   },
   "outputs": [],
   "source": [
    "linear_svm_clf.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:56.653288Z",
     "start_time": "2019-12-02T22:50:56.643575Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = linear_svm_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:56.678145Z",
     "start_time": "2019-12-02T22:50:56.663303Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:56.710360Z",
     "start_time": "2019-12-02T22:50:56.695237Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:56.726987Z",
     "start_time": "2019-12-02T22:50:56.720684Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:56.751541Z",
     "start_time": "2019-12-02T22:50:56.735685Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:56.775536Z",
     "start_time": "2019-12-02T22:50:56.764871Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions == y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:56.794172Z",
     "start_time": "2019-12-02T22:50:56.785369Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:50:56.808004Z",
     "start_time": "2019-12-02T22:50:56.801196Z"
    }
   },
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:01.489450Z",
     "start_time": "2019-12-02T22:50:56.813605Z"
    }
   },
   "outputs": [],
   "source": [
    "ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:01.535585Z",
     "start_time": "2019-12-02T22:51:01.501595Z"
    }
   },
   "outputs": [],
   "source": [
    "ada_pred = ada.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:01.564663Z",
     "start_time": "2019-12-02T22:51:01.541616Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, ada_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras neural net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:01.593179Z",
     "start_time": "2019-12-02T22:51:01.581264Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.metrics import categorical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:01.612759Z",
     "start_time": "2019-12-02T22:51:01.604258Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:05.614903Z",
     "start_time": "2019-12-02T22:51:01.620559Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='softmax', input_dim= X_test.shape[1]))\n",
    "model.add(Dense(units=1)) # one output\n",
    "model.compile(loss='mse', optimizer='RMSprop', metrics= ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_res, y_train_res, epochs=10, batch_size=32)\n",
    "accuracy = history.history['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:05.650941Z",
     "start_time": "2019-12-02T22:50:14.825Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:05.656237Z",
     "start_time": "2019-12-02T22:50:14.834Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_val_score(linear_svm_clf,\n",
    "                X_train, y_train, \n",
    "                cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:05.658270Z",
     "start_time": "2019-12-02T22:50:14.852Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:05.662225Z",
     "start_time": "2019-12-02T22:50:14.862Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:05.667326Z",
     "start_time": "2019-12-02T22:50:14.879Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.head()\n",
    "df_test_unbiased.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:05.673487Z",
     "start_time": "2019-12-02T22:50:14.889Z"
    }
   },
   "outputs": [],
   "source": [
    "df_master = pd.concat([df_train, df_test_unbiased])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:05.677901Z",
     "start_time": "2019-12-02T22:50:14.903Z"
    }
   },
   "outputs": [],
   "source": [
    "df_master.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:05.682493Z",
     "start_time": "2019-12-02T22:50:14.927Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(scaler(), LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:05.716361Z",
     "start_time": "2019-12-02T22:50:14.941Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:05.725154Z",
     "start_time": "2019-12-02T22:50:14.954Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:05.731803Z",
     "start_time": "2019-12-02T22:50:14.961Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:05.736998Z",
     "start_time": "2019-12-02T22:50:14.977Z"
    }
   },
   "outputs": [],
   "source": [
    "preds == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:05.742418Z",
     "start_time": "2019-12-02T22:50:15.021Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T22:51:05.750175Z",
     "start_time": "2019-12-02T22:50:15.057Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, preds) / confusion_matrix(y_test, preds).sum(axis = 0),\n",
    "                             cmap = 'viridis_r', cbar_kws = {'label': 'fraction of predictions'})\n",
    "\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
