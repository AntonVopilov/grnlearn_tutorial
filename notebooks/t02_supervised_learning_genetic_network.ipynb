{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning of  a simple genetic network in *E. coli*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content here is licensed under a CC 4.0 License. The code in this notebook is released under the MIT license. \n",
    "\n",
    "\n",
    "By Manu Flores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the next line if you're in Google Collab \n",
    "#! pip install -r https://raw.githubusercontent.com/manuflores/grnlearn_tutorial/master/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:35.945400Z",
     "start_time": "2019-12-03T04:50:19.560493Z"
    }
   },
   "outputs": [],
   "source": [
    "import grn as g\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import hvplot\n",
    "import hvplot.pandas\n",
    "import holoviews as hv\n",
    "from holoviews import dim, opts\n",
    "import bokeh_catplot\n",
    "import bokeh \n",
    "import bokeh.io\n",
    "from bokeh.io import output_file, save, output_notebook\n",
    "\n",
    "\n",
    "output_notebook()\n",
    "hv.extension('bokeh')\n",
    "np.random.seed(42)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "g.set_plotting_style()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome back ! This is the core of the tutorial. In this notebook we will learn the patterns of a [gene regulatory network](https://en.wikipedia.org/wiki/Gene_regulatory_network), more specifically of the [genetic network of a single regulatory protein (PurR)](https://academic.oup.com/nar/article/39/15/6456/1022585) in *Escherichia coli*. In the last tutorial we extracted the connection of this simple network using data from [RegulonDB](http://regulondb.ccg.unam.mx/menu/about_regulondb/what_is_regulondb/index.jsp). In this tutorial we will continue using data from the Palsson Lab at UCSD. \n",
    "\n",
    "Now that we have extracted the PurR gene network (*PurR [regulon](https://en.wikipedia.org/wiki/Regulon)*), now it's time to prepare the dataset in order to learn the patterns that will enable us to predict new genes that might be inside this biological module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in *E. coli* RNA-seq dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using an RNA-seq dataset from the Palsson Lab published in this [paper](https://academic.oup.com/nar/article/47/5/2446/5304327). This dataset includes more than 50 expression conditions consisting of single gene mutants and laboratory evolution experiments. This dataset is nice because it contains genetic perturbations that represent different cell states and will ideally allow us to infer important biological information from the PurR system in *E. coli*. One last thing to notice is that the data are in [transcript per million](http://www.arrayserver.com/wiki/index.php?title=TPM) units and they were log-transformed before analysis. \n",
    "\n",
    "Let's go ahead and load the dataset into our workspace. Important to notice that if you're running this notebook in Google Colab, you will have to load the dataset directly from the Github url presented below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:36.254330Z",
     "start_time": "2019-12-03T04:50:35.962448Z"
    }
   },
   "outputs": [],
   "source": [
    "# url = 'https://raw.githubusercontent.com/manuflores/grnlearn_tutorial/master/data/palsson_rna_seq.csv'\n",
    "# df = pd.read_csv(url)\n",
    "df = pd.read_csv('../data/palsson_rna_seq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:36.275857Z",
     "start_time": "2019-12-03T04:50:36.262279Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset contains 4K + rows and 100 + columns. In this dataset **each row represent a gene and each column an expression condition**. The only exceptions are the first two first columns correspond to the [locus tag](https://www.wikidata.org/wiki/Property:P2393) and the gene's name. In this sense, each column represents a proxy to the amount of RNA collected for each gene in a given experiment. The same way, you can think of every column as the intensity of \"expression\" of a given gene in multiple growth conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:36.335800Z",
     "start_time": "2019-12-03T04:50:36.287940Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's just divide the annotation and numerical segments of our dataset to continue processing the numerical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:36.370421Z",
     "start_time": "2019-12-03T04:50:36.344921Z"
    }
   },
   "outputs": [],
   "source": [
    "data_ = df.copy()\n",
    "\n",
    "# Extracting the annotation of the dataset (gene information)\n",
    "annot = data_.iloc[:, :2]\n",
    "\n",
    "# Extracting the real numerical data log(TPM)\n",
    "data = data_.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start our data analysis pipeline by normalizing and looking for null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:36.385953Z",
     "start_time": "2019-12-03T04:50:36.380451Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler as scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:36.432564Z",
     "start_time": "2019-12-03T04:50:36.394188Z"
    }
   },
   "outputs": [],
   "source": [
    "ss = scaler()\n",
    "norm_data = ss.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the data has any null entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:37.085786Z",
     "start_time": "2019-12-03T04:50:36.439418Z"
    }
   },
   "outputs": [],
   "source": [
    "norm_data= pd.DataFrame(norm_data, columns = data.columns)\n",
    "norm_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are none. We can quickly verify this using the `pd.notnull` function from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:37.118310Z",
     "start_time": "2019-12-03T04:50:37.100682Z"
    }
   },
   "outputs": [],
   "source": [
    "np.all(pd.notnull(norm_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, we're good to go ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in PurR regulon datasets to annotate our training and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After normalizing our data, we want to make a training and test data sets. Let's load in the data from the last analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:37.146304Z",
     "start_time": "2019-12-03T04:50:37.134688Z"
    }
   },
   "outputs": [],
   "source": [
    "# You know the drill, uncomment if in colab \n",
    "# url_purr_rdb = https://raw.githubusercontent.com/manuflores/grnlearn_tutorial/master/data/purr_regulon_db.csv\n",
    "#purr_regulondb = pd.read_csv(url)\n",
    "purr_regulondb = pd.read_csv('../data/purr_regulon_db.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:37.175693Z",
     "start_time": "2019-12-03T04:50:37.162250Z"
    }
   },
   "outputs": [],
   "source": [
    "# url_purr_hi = https://raw.githubusercontent.com/manuflores/grnlearn_tutorial/master/data/purr_regulon_db.csv\n",
    "#purr_hi = pd.read_csv(url_purr_hi)\n",
    "purr_hi = pd.read_csv('../data/purr_regulon_hitrn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:37.200880Z",
     "start_time": "2019-12-03T04:50:37.189663Z"
    }
   },
   "outputs": [],
   "source": [
    "print('The RegulonDB has %d nodes and the hiTRN has %d nodes \\\n",
    "for the PurR regulon genetic network respectively.'%(purr_regulondb.shape[0], purr_hi.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder in this datasets, the `TG` column represents the **target genes** that are controlled by PurR. In other words, the genes that are directly regulated by the PurR regulator will be in the TG column of this dataframes. \n",
    "\n",
    "Let's extract the TGs as a `np.array` and get the genes that were discovered by the Palsson Lab. These extra genes discovered will serve as our test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:37.224574Z",
     "start_time": "2019-12-03T04:50:37.214892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the target genes of the PurR gene network from RegulonDB\n",
    "purr_rdb_tgs = np.unique(purr_regulondb.tg.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:37.254985Z",
     "start_time": "2019-12-03T04:50:37.237718Z"
    }
   },
   "outputs": [],
   "source": [
    "len(purr_rdb_tgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:37.276204Z",
     "start_time": "2019-12-03T04:50:37.265334Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the target genes of the PurR gene network from the Palsson dataset\n",
    "purr_hi_tgs = np.unique(purr_hi.gene.values)\n",
    "\n",
    "purr_hi_tgs = [gene.lower() for gene in purr_hi_tgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:37.315581Z",
     "start_time": "2019-12-03T04:50:37.299782Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the discovered genes by the Palsson lab \n",
    "new_purr_tgs = set(purr_hi_tgs) - set(purr_rdb_tgs)\n",
    "\n",
    "new_purr_tgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that indeed the hiTRN has 5 more interactions. Let's see if we can accurately predict this interactions directly from the RNA-seq data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing correlation between genes across conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before jumping to applying an ML model to our data, let's proceed to make a simple EDA. As I've said in the presentation the notion that makes this approach biologically plausible is that **genes that are coexpressed are probably corregulated**. A simple proxy for coexpression is correlation across expression conditions. **However, we're not implying that correlation indicates a regulatory interaction.** This is just to get a feel of the data.\n",
    "\n",
    "Let's make a couple of plots to see that indeed the test genes that we're looking for are correlated with purr, and if this relationship looks linear. We'll use the Seaborn library in this case because it has a nice feat that allows to embed a statistical function into the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:37.347814Z",
     "start_time": "2019-12-03T04:50:37.329318Z"
    }
   },
   "outputs": [],
   "source": [
    "def corr_plot(data, gene_1, gene_2):\n",
    "    \"\"\"\n",
    "    Scatter plot to devise correlation. \n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    * data(pd.DataFrame): Input dataframe that contains for which to pull out data. \n",
    "    \n",
    "    * gene_x (str): gene_name of the genes to visualize.\n",
    "    \n",
    "    Returns \n",
    "    ---------\n",
    "    * fig (plt.figure) : sns.jointplot hardcoded to be a scatterplot of the genes. \n",
    "    \n",
    "    \"\"\"\n",
    "    gene_1_data  = data[data['gene_name'] == gene_1]\n",
    "    \n",
    "    assert gene_1_data.shape[0] ==1, 'Gene 1 not in dataset'\n",
    "    \n",
    "    gene_1_vals =  gene_1_data.iloc[:, 3:].values.T\n",
    "    \n",
    "    gene_2_data  = data[data['gene_name'] == gene_2]\n",
    "    \n",
    "    assert gene_2_data.shape[0] ==1, 'Gene 2 not in dataset'\n",
    "    \n",
    "    gene_2_vals =  gene_2_data.iloc[:, 3:].values.T\n",
    "    \n",
    "    df_plot = pd.DataFrame({gene_1: gene_1_vals.flatten(),\n",
    "                            gene_2 : gene_2_vals.flatten()})\n",
    "    \n",
    "    plt.figure(figsize = (6, 4))\n",
    "    fig = sns.jointplot(data = df_plot, \n",
    "                  x = gene_1,\n",
    "                  y = gene_2,\n",
    "                  stat_func = pearsonr,\n",
    "                  alpha = 0.5,\n",
    "                  color = 'dodgerblue');\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now iterate over the putative TGs and plot them against PurR. In the following plots, each dot represents the expression level (in normalized log(TPM) units), a proxy for the number of mRNA counts for a given gene) of both genes in a specific expression condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:44.636735Z",
     "start_time": "2019-12-03T04:50:37.357902Z"
    }
   },
   "outputs": [],
   "source": [
    "for new_tg in new_purr_tgs: \n",
    "    \n",
    "    corr_plot(df, 'purr', new_tg);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some, but not all the genes are strongly correlated with PurR. This is normal because the TRN has a lot of feedback so it could be that despite that PurR regulates a given gene, there are potentially other TFs controlling those target genes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter noise using PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis is a widely used technique in unsupervised learning to perform dimensionality reduction (if you want to know more about it I highly recommend this [blog post](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html) by Sebas Raschka). One can also use PCA as a \"noise reduction\" technique because projecting into a (smaller) latent space and reconstructing the dataset from this space with smaller dimensionality forces the algorithm to learn important features of the data. Specifically the latent space (the principal components) will maximize the variance across the dataset. \n",
    "\n",
    "First, let's explore the dimensionality of our RNA-seq dataset using PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:44.649724Z",
     "start_time": "2019-12-03T04:50:44.642817Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:44.769235Z",
     "start_time": "2019-12-03T04:50:44.660411Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca = pca.fit(norm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:45.152403Z",
     "start_time": "2019-12-03T04:50:44.776361Z"
    }
   },
   "outputs": [],
   "source": [
    "cum_exp_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# look at it\n",
    "plt.figure(figsize = (6,4))\n",
    "plt.plot(cum_exp_var*100, color = 'dodgerblue') #because LA\n",
    "plt.xlabel('Number of dimensions', fontsize= 16)\n",
    "plt.ylabel('Cumulative variance percentage', fontsize = 16)\n",
    "plt.title('PCA Explained Variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:45.175464Z",
     "start_time": "2019-12-03T04:50:45.163738Z"
    }
   },
   "outputs": [],
   "source": [
    "print('The first five principal components explain %.2f of the variance in the dataset.'%cum_exp_var[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset is of very small dimensionality. We can now project into this subspace that contains 95% of the variance and reconstruct the denoised dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:45.274692Z",
     "start_time": "2019-12-03T04:50:45.186295Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(0.95).fit(norm_data)\n",
    "latent = pca.transform(norm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:45.297787Z",
     "start_time": "2019-12-03T04:50:45.286130Z"
    }
   },
   "outputs": [],
   "source": [
    "reconstructed = pca.inverse_transform(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:45.317535Z",
     "start_time": "2019-12-03T04:50:45.311176Z"
    }
   },
   "outputs": [],
   "source": [
    "recon_df= pd.DataFrame(reconstructed, columns = data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:45.344409Z",
     "start_time": "2019-12-03T04:50:45.328965Z"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[:, :2].shape, recon_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:45.366570Z",
     "start_time": "2019-12-03T04:50:45.353476Z"
    }
   },
   "outputs": [],
   "source": [
    "recon_df_ = pd.concat([df.iloc[:, :2], recon_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:45.437261Z",
     "start_time": "2019-12-03T04:50:45.394194Z"
    }
   },
   "outputs": [],
   "source": [
    "recon_df_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize correlation again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the correlation of the target genes we want to discover using the denoised dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:52.518468Z",
     "start_time": "2019-12-03T04:50:45.446234Z"
    }
   },
   "outputs": [],
   "source": [
    "for new_tg in new_purr_tgs: \n",
    "    \n",
    "    corr_plot(recon_df_, 'purr', new_tg);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the reconstructed space, we've constrained the data to have a bigger covariance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize in PCA space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we already have the projection of our dataset into a smaller dimension, we can also visualize all of the genes in the first two principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:52.644880Z",
     "start_time": "2019-12-03T04:50:52.523674Z"
    }
   },
   "outputs": [],
   "source": [
    "hv.Points((latent[: , 0], latent[: , 1])).opts(xlabel = 'principal component 1',\n",
    "                                               ylabel = 'principal component 2',\n",
    "                                               color = '#1E90FF', \n",
    "                                               size = 5, \n",
    "                                               alpha = 0.15, \n",
    "                                               padding = 0.1, \n",
    "                                               width = 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot really see a specific structure in the first two components. Maybe a non-linear dimensionality reduction technique such as UMAP could do a better job to get the clusters in higher dimensions. We'll come back to that in the next tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have preprocessed our data we can proceed to annotate it. Specifically we want to label our data for each gene, if its inside the PurR regulon or not. \n",
    "\n",
    "First-off, let's generate our test set. We'll use a helper function that let's us filter from the dataframe. We also have the function in the `grn` module in this folder if you want to use it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:52.662442Z",
     "start_time": "2019-12-03T04:50:52.650359Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_gene_data(data, gene_name_column, test_gene_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extract data from specific genes given a larger dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    \n",
    "    * data (pd.DataFrame): large dataframe from where to filter.\n",
    "    * gene_name_column (str): column to filter from in the dataset.\n",
    "    * test_gene_list (array-like) : a list of genes you want to get. \n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    * gene_profiles (pd.DataFrame) : dataframe with the genes you want\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    gene_profiles = pd.DataFrame()\n",
    "\n",
    "    for gene in data[gene_name_column].values:\n",
    "\n",
    "        if gene in test_gene_list: \n",
    "\n",
    "            df_ = data[(data[gene_name_column] == gene)]\n",
    "\n",
    "            gene_profiles = pd.concat([gene_profiles, df_])\n",
    "    \n",
    "    gene_profiles.drop_duplicates(inplace = True)\n",
    "    \n",
    "    return gene_profiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a one hot encoded vector that corresponds to being an element of the PurR regulon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:52.686266Z",
     "start_time": "2019-12-03T04:50:52.673709Z"
    }
   },
   "outputs": [],
   "source": [
    "one_hot = [1 if row  in purr_hi_tgs else 0 for  row in  recon_df_['gene_name'].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add the one hot vector to the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:52.704724Z",
     "start_time": "2019-12-03T04:50:52.694643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Appending the one hot vector to the dataset\n",
    "recon_df_['output'] = one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:52.755826Z",
     "start_time": "2019-12-03T04:50:52.711601Z"
    }
   },
   "outputs": [],
   "source": [
    "recon_df_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go ahead and make the test set using the `get_gene_data` function and the TG list to discover. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:52.769961Z",
     "start_time": "2019-12-03T04:50:52.763712Z"
    }
   },
   "outputs": [],
   "source": [
    "test_purr_tgs  = list(new_purr_tgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:52.853760Z",
     "start_time": "2019-12-03T04:50:52.778566Z"
    }
   },
   "outputs": [],
   "source": [
    "test = get_gene_data(recon_df_, 'gene_name', test_purr_tgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:52.907839Z",
     "start_time": "2019-12-03T04:50:52.861691Z"
    }
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop these test genes from the reconstructed dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:52.944084Z",
     "start_time": "2019-12-03T04:50:52.920889Z"
    }
   },
   "outputs": [],
   "source": [
    "recon_df_non_regulon = recon_df_.copy().drop(test.index.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Finally, let's go ahead and add some \"noise\" to our test dataset, in the sense that we need to test if our algorithm can point out negative examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:52.972809Z",
     "start_time": "2019-12-03T04:50:52.954119Z"
    }
   },
   "outputs": [],
   "source": [
    "noise = recon_df_non_regulon.sample(n = 30, replace = False,\n",
    "                         axis = 0, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge both of this dataframes to get an \"unbiased test set\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:52.990545Z",
     "start_time": "2019-12-03T04:50:52.979006Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_unb = pd.concat([test, noise]) ## unbiased test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:53.008083Z",
     "start_time": "2019-12-03T04:50:52.996444Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_unb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:53.030813Z",
     "start_time": "2019-12-03T04:50:53.016264Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_unbiased = df_test_unb.copy().reset_index(drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:53.094462Z",
     "start_time": "2019-12-03T04:50:53.041358Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_unbiased.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:53.114326Z",
     "start_time": "2019-12-03T04:50:53.104043Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_unbiased.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:53.129990Z",
     "start_time": "2019-12-03T04:50:53.121633Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = recon_df_non_regulon.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - test split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, we're almost there. Literally all we have to do now is just divide into the training data and the target that will be the output of our supervised learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:53.228590Z",
     "start_time": "2019-12-03T04:50:53.141132Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.head()\n",
    "df_test_unbiased.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:53.258250Z",
     "start_time": "2019-12-03T04:50:53.240207Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.shape\n",
    "df_test_unbiased.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:53.282477Z",
     "start_time": "2019-12-03T04:50:53.266278Z"
    }
   },
   "outputs": [],
   "source": [
    "# Divide our training set X -> input and y-> output datasets\n",
    "X_train = df_train.iloc[:, 2: -1].values\n",
    "y_train = df_train.iloc[:,  -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:53.308165Z",
     "start_time": "2019-12-03T04:50:53.291195Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check we did it right\n",
    "X_train[:5, :5]\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:53.328410Z",
     "start_time": "2019-12-03T04:50:53.315858Z"
    }
   },
   "outputs": [],
   "source": [
    "# Divide our test set too\n",
    "X_test = df_test_unbiased.iloc[:, 2:-1].values\n",
    "\n",
    "y_test = df_test_unbiased.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:53.352770Z",
     "start_time": "2019-12-03T04:50:53.335748Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test[:5, :5]\n",
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, we're ready to try out different models ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance dataset using SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or are we? One last thing that we might want to check (and correct for) is if we have a so-called balanced training set, i.e. if we have the same number of positive (inside genetic network) and negative (not in gene network) examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:53.374064Z",
     "start_time": "2019-12-03T04:50:53.358612Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:53.393054Z",
     "start_time": "2019-12-03T04:50:53.379495Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we don't - because the PurR gene network only contains a tiny fraction of the whole *E. coli* genome. Luckily there are great libraries out there that can help us to balance our dataset. One of such libraries is [`imbalanced-learn`](https://imbalanced-learn.readthedocs.io/). I highly recommend this library! It is super well documented and has some really cool algorithms to over/undersample. Because we have just a tiny bit of data for the positive examples, we'll go with oversampling. A classic algorithm to do this is called SMOTE and it is based on generating new datapoints using a kNN like procedure of the positive samples. Let's transform our dataset! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:53.575847Z",
     "start_time": "2019-12-03T04:50:53.399064Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#resampling is done on training dataset only\n",
    "X_train_res, y_train_res = SMOTE(random_state = 42).fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check that our dataset is indeed balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:53.605832Z",
     "start_time": "2019-12-03T04:50:53.590184Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(y_train_res).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! This time for sure, let's apply some ML to check if we can really learn new nodes for our gene network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using supervised learning models to learn the PurR regulon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, a good thing at this point would be to apply a bunch of models, see which one performs the best within certain criterion (model complexity, runtime, etc.). Afterwards we would do some type of hyperparameter tuning and cross-validation to make our final model. \n",
    "\n",
    "The approach we're going to take though is the following: because we know *a priori* this is a simple genetic network and, we have a decent amount of data, we'll try a linear model first, specifically a linear Support Vector Machine. Afterwards we'll try some non-linear models like Random Forest classifier and a neural network. \n",
    "\n",
    "We will use the scikit learn and keras libraries for this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out a linear classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import our linear SVM and check it's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:53.621350Z",
     "start_time": "2019-12-03T04:50:53.614396Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:53.639231Z",
     "start_time": "2019-12-03T04:50:53.631638Z"
    }
   },
   "outputs": [],
   "source": [
    "linear_svm_clf = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:57.894753Z",
     "start_time": "2019-12-03T04:50:53.645791Z"
    }
   },
   "outputs": [],
   "source": [
    "linear_svm_clf.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:57.912089Z",
     "start_time": "2019-12-03T04:50:57.902679Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = linear_svm_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:57.930640Z",
     "start_time": "2019-12-03T04:50:57.920510Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:57.952392Z",
     "start_time": "2019-12-03T04:50:57.941013Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:57.966928Z",
     "start_time": "2019-12-03T04:50:57.959656Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:57.989055Z",
     "start_time": "2019-12-03T04:50:57.973645Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:58.003844Z",
     "start_time": "2019-12-03T04:50:57.994883Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions == y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the linear model does quite well as we expected. At this point I would just tune the hyperparams of the model and stick to it. However, let's just try other models to have a comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:58.015697Z",
     "start_time": "2019-12-03T04:50:58.010155Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:50:58.029193Z",
     "start_time": "2019-12-03T04:50:58.023269Z"
    }
   },
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:51:02.362524Z",
     "start_time": "2019-12-03T04:50:58.034974Z"
    }
   },
   "outputs": [],
   "source": [
    "ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:51:02.393777Z",
     "start_time": "2019-12-03T04:51:02.367794Z"
    }
   },
   "outputs": [],
   "source": [
    "ada_pred = ada.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:51:02.418809Z",
     "start_time": "2019-12-03T04:51:02.400236Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, ada_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! The random forest does reaaaally well. However though, we might be worried that this model actually is overfitting the data. In this sense I would trust the LinearSVM more and discard this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras neural net. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try out a neural network model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:51:02.435798Z",
     "start_time": "2019-12-03T04:51:02.425399Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.metrics import categorical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:51:02.472748Z",
     "start_time": "2019-12-03T04:51:02.451126Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:51:19.143553Z",
     "start_time": "2019-12-03T04:51:02.481360Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='softmax', input_dim= X_test.shape[1]))\n",
    "model.add(Dense(units=1)) # one output\n",
    "model.compile(loss='mse', optimizer='RMSprop', metrics= ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_res, y_train_res, epochs=10, batch_size=32)\n",
    "accuracy = history.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:51:34.516412Z",
     "start_time": "2019-12-03T04:51:34.507437Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case a simple neural network with one hidden layer and 64 neurons does pretty well and doesn't overfit our data. We could alternatively go with this model, but just for the sake of this tutorial, let's continue sticking with our LinearSVM. In practice you could continue with either one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, let's perform cross-validation on our linear model to be confident about it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:51:19.183233Z",
     "start_time": "2019-12-03T04:51:19.173620Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:53:14.768530Z",
     "start_time": "2019-12-03T04:53:12.470188Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_val_score(linear_svm_clf,\n",
    "                X_train, y_train, \n",
    "                cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it performs pretty good in the cross validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to take this all the way to the finish line, let's do a simple pipeline that normalizes and applies the LinearSVM. With this we will tell that doing the noise reduction is not essential for our classification purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:55:31.942489Z",
     "start_time": "2019-12-03T04:55:31.934661Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:55:34.231246Z",
     "start_time": "2019-12-03T04:55:34.154508Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.head()\n",
    "df_test_unbiased.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:55:36.125324Z",
     "start_time": "2019-12-03T04:55:36.112405Z"
    }
   },
   "outputs": [],
   "source": [
    "df_master = pd.concat([df_train, df_test_unbiased])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:55:36.189936Z",
     "start_time": "2019-12-03T04:55:36.136013Z"
    }
   },
   "outputs": [],
   "source": [
    "df_master.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:55:39.844889Z",
     "start_time": "2019-12-03T04:55:39.837845Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(scaler(), LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:55:39.877765Z",
     "start_time": "2019-12-03T04:55:39.853379Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:55:40.588976Z",
     "start_time": "2019-12-03T04:55:39.885113Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:55:40.612223Z",
     "start_time": "2019-12-03T04:55:40.600344Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:55:49.519618Z",
     "start_time": "2019-12-03T04:55:49.506814Z"
    }
   },
   "outputs": [],
   "source": [
    "preds == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:55:49.540562Z",
     "start_time": "2019-12-03T04:55:49.528227Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T04:55:54.502519Z",
     "start_time": "2019-12-03T04:55:54.230549Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, preds) / confusion_matrix(y_test, preds).sum(axis = 0),\n",
    "                             cmap = 'viridis_r', cbar_kws = {'label': 'fraction of predictions'})\n",
    "\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
